
<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.3.0 -->
<title> Playing StarCraft II with Reinforcement Learning</title>
<meta property="og:title" content="ECS170 project" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Playing StarCraft II with Reinforcement Learning" />
<meta property="og:description" content="Playing StarCraft II with Reinforcement Learning" />
<link rel="canonical" href="https://telecombcn-dl.github.io/2017-dlai-team5/" />
<meta property="og:url" content="https://telecombcn-dl.github.io/2017-dlai-team5/" />
<meta property="og:site_name" content="2017-dlai-team5" />
<script type="application/ld+json">
{"name":"ECS 170 project",
"description":"Playing StarCraft II with Reinforcement Learning",
"author":null,
"@type":"WebSite","url":"https://telecombcn-dl.github.io/2017-dlai-team5/",
"image":null,"publisher":null,"headline":"DLAI 2017 - Project Work Group 5 :",
"dateModified":null,"datePublished":null,
"sameAs":null,"mainEntityOfPage":null,"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="description" content="Playing StarCraft II with Reinforcement Learning"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/Users/shraddhaagrawal/Desktop/SwaggerBot/style.css?v=eb8a799b4a54b568f31fc9a734ea1a499558e4b4">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">SwaggerBot</h1>
      <h2 class="project-tagline">Playing StarCraft II with Reinforcement Learning</h2>
      
        <a href="https://github.com/okdolly/SwaggerBot-StarCraft2" class="btn">View on GitHub</a>
      
      
    </section>

    <section class="main-content">
      <h1 id="dlai-2017---project-work-group-5-">ECS 170 project</h1>
<h1 id="playing-starcraft-ii-with-reinforcement-learning">Playing StarCraft II with Reinforcement Learning</h1>
<p>The Team is made up by:</p>
<img src="Team2.jpg" alt="he" />
<table>
    <tr>
      <td>Shraddha Agrawal</td>
      <td >Dolly Ye</td>
      <td >Aria</td>
      <td >Axel</td>
    </tr>
</table>

<p>This project was developed during the <a href="https://github.com/dr-jam/ECS170">ECS 170:Artificial Intelligence</a> Course at UC Davis, Spring 2018.</p>


<h2 id="starcraft-ii">StarCraft II</h2>
<p align="center">
<img src="img-sc2-logo--large.png" alt="" />
</p>

<p style="text-align: justify">
As defined on the Blizzard website (the company that develops the game):
</p>

<blockquote>
  <p>StarCraft II: Wings of Liberty is the long-awaited sequel to the original StarCraft, Blizzard Entertainment’s critically acclaimed sci-fi real-time strategy (RTS) game. StarCraft II: Wings of Liberty is both a challenging single-player game and a fast-paced multiplayer game.
In typical real-time strategy games, players build armies and vie for control of the battlefield. The armies in play can be as small as a single squad of Marines or as large as a full-blown planetary invasion force. As commander, you observe the battlefield from a top-down perspective and issue orders to your units in real time. Strategic thinking is key to success; you need to gather information about your opponents, anticipate their moves, outflank their attacks, and formulate a winning strategy.</p>
</blockquote>

<p style="text-align: justify">
It combines fast paced micro-actions with the need for high-level planning and execution. Over the previous two decades, StarCraft I and II have been pioneering and enduring e-sports, 2 with millions of casual and highly competitive professional players. Defeating top human players therefore becomes a meaningful and measurable long-term objective.
</p>

<p style="text-align: justify">
From a reinforcement learning perspective, StarCraft II also offers an unparalleled opportunity to explore many challenging new frontiers:
</p>

<ol>
  <li>It is a multi-agent problem in which several players compete for influence and resources. It is also multi-agent at a lower-level: each player controls hundreds of units, which need to collaborate to achieve a common goal.</li>
  <li>It is an imperfect information game. The map is only partially observed via a local camera, which must be actively moved in order for the player to integrate.</li>
</ol>

<h3 id="pysc2-environment">PySC2 Environment</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="images/logo_sc2.png" width="200" alt="" /></th>
      <th style="text-align: center"><img src="images/logo_deepmind.png" width="200" alt="" /></th>
      <th style="text-align: center"><img src="images/logo_python.png" width="200" alt="" /></th>
      <th style="text-align: center"><img src="images/logo_tensorflow.png" width="200" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">StarCarft II</td>
      <td style="text-align: center">Google DeepMind</td>
      <td style="text-align: center">Python</td>
      <td style="text-align: center">TensorFlow</td>
    </tr>
  </tbody>
</table>

<p style="text-align: justify">
PySC2 is DeepMind's Python component of the StarCraft II Learning Environment (<strong>SC2LE</strong>). It exposes Blizzard Entertainment's StarCraft II Machine Learning API as a Python reinforcement learning (<strong>RL</strong>) Environment. 
This is a collaboration between DeepMind and Blizzard to develop StarCraft II into a rich environment for RL research. PySC2 provides an interface for RL agents to interact with StarCraft 2, getting observations and rewards and sending actions.
</p>

<p style="text-align: justify">
Scheme 1 explains how SC2LE works combining StarCarft II API with Google DeepMind Libraries:
</p>

<p align="center">
  <img src="images/sc2le.png" width="1000" alt="" />
  <br />
  <br />
  Fig. 1: SC2LE. Source: [1].
</p>

<h3 id="objectives">Objectives</h3>

<p style="text-align: justify">
Playing the whole game is quite an ambitious goal that currently is only whithin the reach of scripted agents. However, the StarCraft II learning environment provides several challenges that are most appropriate to test the learning capabilities of an intelligent agent. It is our intention to develop an intelligent Deep RL agent that can perform successfully on several mini-games with bound objectives. Moreover, we want to experiment with the reward system to see how several changes may influence the behaviour of the agent. That's why we can define our objectives by:
</p>

<ul>
  <li>Focusing on small mini-games.</li>
  <li>Training and evaluating several RL agents.</li>
  <li>Dealing and try to improve the reward system (reward "hacking").</li>
</ul>

<h2 id="techniques">Techniques</h2>

<h3 id="learning-curve">Learning Curve</h3>

<p style="text-align: justify">
Before starting to train the SC2 agents in pysc2 environment, we went through a series of <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0">tutorials</a>, which implement in <a href="https://www.tensorflow.org/">TensorFlow</a> the different RL algorithms applied to the <a href="https://gym.openai.com/envs/">OpenAI GYM</a> environment.
</p>

<p style="text-align: justify">
These were the some of the topics we went over: 
</p>

<ul>
  <li><a href="http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/">Demystifying deep reinforcement learning</a></li>
  <li><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0">Q-Learning with Tables and Neural Networks</a></li>
  <li><a href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149">Two-armed Bandit</a></li>
  <li><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c">Contextual Bandits</a></li>
  <li><a href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724">Policy-based Agents</a></li>
  <li><a href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99">Model-Based RL</a></li>
  <li><a href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df">Deep Q-Networks and Beyond</a></li>
  <li><a href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a">Visualizing an Agent’s Thoughts and Actions</a></li>
  <li><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc">Partial Observability and Deep Recurrent Q-Networks</a></li>
  <li><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf">Action-Selection Strategies for Exploration</a></li>
  <li><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2">Asynchronous Actor-Critic Agents (A3C)</a></li>
</ul>

<p align="center">
  <img src="images/learning_curve.gif" width="1000" alt="" />
</p>

<h3 id="a3c">Q-Learning</h3>

<p style="text-align: justify">

Q-Learning is a popular reinforcement algorithm used in StarCraft II agent. We built our agent on top of Q-learning implementation: <a href="https://github.com/xhujoy/pysc2-agents"> one by steven brown</a>; and <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents-notebooks"> by Maravn Zhou </a></p>

<h3 id="a3c">Deep-Q-Learning</h3>

<p style="text-align: justify">

D-Q-Learning is a popular reinforcement algorithm used in StarCraft II agent. We first spent time understanding this tutorial on how to use pytorch to train a DQN agent on the  <a href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"> one by Adam Paszke</a>; and <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents-notebooks"> by Maravn Zhou </a>. Deep Q network is built on one simple layer of Q-network.
</p>


<h4 id="theoretical-foundations">Theoretical Foundations</h4>



<p style="text-align: justify">
Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, <strong>Q-Learning </strong>attempts to learn the value of being in a given state, and taking a specific action there.
</p>

<p style="text-align: justify">
 Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly. our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state.

</p>

<p style="text-align: justify">
In order to go from Q-learning to DQN, these are the following things we implement:
<ul>
<li>Going from a single-layer network to a multi-layer convolutional network. </li>
<li> Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience.</li>
<li> Utilizing a second “target” network, which we will use to compute target Q-values during our updates.</li>
</ul>

</p>

<p style="text-align: justify">
The transition is depicted here:
</p>

<p align="center">
  <img src="table.png" alt="" />
  <br />
  <br />
  Fig. 2: Getting from Q-Network to Deep Q-Network. Source [4].
</p>


<h2 id="training-rl-agents">Training RL agents</h2>

<h3 id="objective">Objective</h3>

<p style="text-align: justify">
It is our objective to experiment with Reinforcement Learning techniques and analyze their suitability for the StarCraft II game. We bound our scope to one of the mini-games from the Deep Mind's suite: Find and Destroy Zerglings. We believe that a top performer player for this game should display a good balance between exploration and combat skills. As for the algorithm, as previously said, we use A3C, encoding the policy with AtariNet [5] (Fig. 3).
</p>

<p align="center">
  <img src="images/Atari-net.png" width="400" alt="" />
  <br />
  <br />
  Fig. 3: Atari-Net. Source [1].
</p>

<p style="text-align: justify">
The chosen mini-games consists in hunting down Zerglings on a small map using exclusively 3 marines. The position of the Zerglings is unknown a priori, so the player has to explore the map in order to find them. One of the main challenges of this mini-game is that initially there are 19 Zerglings, and enemies respawn only when the previous wave has been cleared. This means that as the episode progresses and the marines kill Zerglings, it gets increasingly difficult to find the next one. We will see that this has a serious impact in the performance of the agent.
</p>

<h3 id="initial-set-up-and-results">Initial set-up and results</h3>

<p style="text-align: justify">
In order to get some sort of tentative results we executed the Hu's implementation in a Google Cloud server for 7 days, with the following specifications:
</p>

<ul>
  <li>2 K80 GPUs</li>
  <li>4 virtual cores</li>
  <li>10GB of RAM</li>
</ul>

<p style="text-align: justify">
The results were kind of underwhelming: we managed to simulate just around 180000 episodes, stalling at an accumulated reward around 20. At this point, it is important to highlight the fact that this reward is very hard to surpass because of the strict respawn rules imposed by the mini-game. Moreover, it is very difficult to train for this kind of scenarios under the no-memory assumption adopted by Markov Decision Processes (MDP), the underlying mathematical formalism in which RL is based on (although DeepMind has also used LSTMs in one of their network, overcoming this issue up to certain extend but moving away from the theoretical guarantees provided by MDPs). Of course, as shown by DeepMind's results, the agent can develop an effective strategy for exploring this map even under the no memory assumption. Fig. 4 shows a comparative between our results and theirs. One of the main facts that can be inferred from this chart is that DeepMind has spent a very high amount of resources in obtaining such good results. Our results are too far from this level of performance, but then again, we cannot afford 600M of simulations.
</p>

<p align="center">
  <img src="images/dm_vs_hu180k.png" width="900" alt="" />
  <br />
  <br />
  Fig. 4: (Left) Results after 7 days of training vs (Right) DeepMind results. At the right hand side, the colors represent: (1) Light red, Atari-Net; (2) Light blue, Fully Conv.; (3) Dark blue, Fully Conv. with LSTM.
</p>

<p style="text-align: justify">
Interestingly enough, it is possible to observe that even in some of the configurations employed by DeepMind (namely the one with LSTM and AtariNet) they still experience the same issue with the plateau around 20 at the begining of the training. After 100M of episodes the results remain fairly stable.
</p>

<p style="text-align: justify">
The main conclusion we draw from this is that training for Reinforcement Learning is very expensive, both in terms of time and money:
</p>

<ul>
  <li>We have estimated that we were simulating near to 26000 episodes per day. At this pace, we would need more than 23000 days (more than 60 years) to complete 600M of episodes (or 10 years for just 100M).</li>
  <li>On the other hand, the estimate of the cost of such training is around $643,963 (Google Cloud fees: $1.15 per hour).</li>
</ul>

<h2 id="next-step-hacking-the-reward">Next step: hacking the reward</h2>

<p style="text-align: justify">
We introduced domain specific knowledge as follows:
</p>

<ul>
  <li>We keep track of the minimum distance from the marines to the Zerglings. Each time this distance increases, we add a small penalty (negative reward) to discourage the marines from fleing, and add a small positive reward when they approach an enemy.</li>
  <li>We believe that correctly managing the camera movements greatly influences the performance of the game, since having it centered in the region of interest (the region where most visible units are located) we increase the chance of hitting the right actions. The rationale is that there are actions for moving and attacking both in the minimap and in the main screen, and if the marines and Zerglings are outside the region covered by the camera, only the actions performed in the minimap are useful for killing Zerglings. For this reason, we add a small reward when the visible region covered by the camera increases after a camera movement.</li>
</ul>

<p style="text-align: justify">
We can see at Fig. 5 that this has indeed a positive effect in the learning speed of the agent. However, we are not sure of the long term effects if we left the algorithm running for more time.
</p>

<p align="center">
  <img src="images/Comparison_GPU.png" width="500" alt="" />
  <br />
  <br />
  Fig. 5: Agent that uses modified ("hacked") rewards vs original agent. Of course, for the sake of comparing both executions in fair conditions only the original rewards (the ones given originally by the environment) are shown. The modified rewards are used internally to update the network.
</p>

<p align="center">
In addition to this, we have performed several executions to analyze the effect of the learning rate upon the performance of the algorithm with modified rewards. The results are shown in Fig. 6.
</p>

<p align="center">
  <img src="images/Learning_rate_comparison.png" width="500" alt="" />
  <br />
  <br />
  Fig. 6: Different executions varying the learning rate. The executions corresponding to the green and blue curves where halted prematurely because the seemed rather unpromissing. On the other hand, the algorithm seems to perform well enough with a learning rate of 0.0001 and 0.00001. We cannot conclude which will behave better in the long run.
</p>

<h2 id="results-and-conclusions">Results and Conclusions</h2>

<p style="text-align: justify">
Of course, we have not achieved Deep Mind's performance, which is beyond our reach in terms of resources. More importantly, our work has been more about exploration, researching and learning rather than about constructing new techniques and pushing forward the state of the art. Along the way, we have introduced ourselves into the fascinating world of Reinforcement Learning ("the cherry on the top"), from which we did not know anything initially, or had very high-level understanding.
</p>

<p style="text-align: justify">
We have applied the concept of "reward hacking" to speed-up the learning of the agent with problem specific information. Of course, this is a double-edge sword since on the one hand we can enrich the otherwise sparse reward with domain-specific knowledge, avoiding the many trials-and-errors of less informed search; but on the other hand we have to manipulate the reward to include this domain knowledge, which can be very difficult, and loses a great deal of the appeal and generality of ready-to-go RL algorithms. It is our belief, however, that using RL alone to play StarCraft II will be very challenging and perhaps unfeasible with the current state of the art, unless it is combined with other technique, or used as a subsystem of a more complex artificial intelligence player.
</p>

<p style="text-align: justify">
In our experiments (somewhat limited because of our resources), we have found that the default learning rate (0.0001) in the algorithms we tested is indeed well chosen: bigger learning rates result in unstable learning problems while lower learning rates result in very slow learning. On the other hand, tweaking the reward to include domain specific knowledge has resulted in mixed results. While the performance seems to grow faster at the beginning, it stalls around 19. In the mini-game we chose, 19 turns out to be a quite critical score because enemies do not respawn until the first wave has been cleared out, which requires an efficient exploration technique.
</p>

<p style="text-align: justify">
All in all, we believe this to be a fun and challenging task for those interested in the realm of artificial intelligence. In the future we seek to keep learning about this topic and add our own contributions to the field, or apply it to other tasks. We also believe that RL has very promising real-world applications, like assistive robotics or automation and control.
</p>

<h3 id="example-video-of-a-trained-agent">Example video of a trained agent</h3>
<p><a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=uMiJt4UcZyo " target="_blank"><img align="center" src="http://img.youtube.com/vi/uMiJt4UcZyo/0.jpg" alt="RL Agents" width="240" height="180" border="10" /></a></p>

<h2 id="references">References</h2>

<ol>
  <li>Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., … Tsing, R. (2017). StarCraft II: A New Challenge for Reinforcement Learning, <a href="https://doi.org/https://deepmind.com/documents/110/sc2le.pdf">link</a></li>
  <li>Simple Reinforcement Learning with Tensorflow series, Arthur Juliani, <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0">link</a> (Accessed: 2017/12/11)</li>
  <li>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. ICML, 2016, <a href="http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf">link</a></li>
  <li>Szepesvári, C. (2010). Algorithms for reinforcement learning. Synthesis lectures on artificial intelligence and machine learning, <a href="http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009">link</a></li>
  <li>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning, 1–9, <a href="https://doi.org/10.1038/nature14236">link</a></li>
</ol>


    </section>

    
  </body>
</html>