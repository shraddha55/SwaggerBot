
<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.3.0 -->
<title> Playing StarCraft II with Reinforcement Learning</title>
<meta property="og:title" content="ECS170 project" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Playing StarCraft II with Reinforcement Learning" />
<meta property="og:description" content="Playing StarCraft II with Reinforcement Learning" />
<link rel="canonical" href="https://telecombcn-dl.github.io/2017-dlai-team5/" />
<meta property="og:url" content="https://telecombcn-dl.github.io/2017-dlai-team5/" />
<meta property="og:site_name" content="2017-dlai-team5" />
<script type="application/ld+json">
{"name":"ECS 170 project",
"description":"Playing StarCraft II with Reinforcement Learning",
"author":null,
"@type":"WebSite","url":"https://telecombcn-dl.github.io/2017-dlai-team5/",
"image":null,"publisher":null,"headline":"DLAI 2017 - Project Work Group 5 :",
"dateModified":null,"datePublished":null,
"sameAs":null,"mainEntityOfPage":null,"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="description" content="Playing StarCraft II with Reinforcement Learning"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="style.css?v=eb8a799b4a54b568f31fc9a734ea1a499558e4b4">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">SwaggerBot</h1>
      <h2 class="project-tagline">Playing StarCraft II with Reinforcement Learning</h2>
      
        <a href="https://github.com/okdolly/SwaggerBot-StarCraft2" class="btn">View on GitHub</a>
      
      
    </section>

    <section class="main-content">
<h1 id="playing-starcraft-ii-with-reinforcement-learning">Playing StarCraft II with Reinforcement Learning</h1>
<p>StarCraft2 Swaggers</p>
<img src="Team2.jpg" alt="he" />
<table>
    <tr>
      <td >" "</td>
      <td >Dolly Ye</td>
      <td >Shraddha Agrawal</td>
      <td >Aria Chang</td>
      <td >Axel A Hernandez</td>
  
      <td >Rhys Li(not here)</td>
    </tr>
</table>




<h2 id="starcraft-ii">StarCraft II</h2>
<p align="center">
<img src="img-sc2-logo--large.png" alt="" />
</p>

<p style="text-align: justify">
As defined on the Blizzard website (the company that develops the game):
</p>

<blockquote>
  <p>StarCraft II: Wings of Liberty is the long-awaited sequel to the original StarCraft, Blizzard Entertainment’s critically acclaimed sci-fi real-time strategy (RTS) game. StarCraft II: Wings of Liberty is both a challenging single-player game and a fast-paced multiplayer game.
In typical real-time strategy games, players build armies and vie for control of the battlefield. The armies in play can be as small as a single squad of Marines or as large as a full-blown planetary invasion force. As commander, you observe the battlefield from a top-down perspective and issue orders to your units in real time. Strategic thinking is key to success; you need to gather information about your opponents, anticipate their moves, outflank their attacks, and formulate a winning strategy.</p>
</blockquote>

<p style="text-align: justify">
It combines fast paced micro-actions with the need for high-level planning and execution. Over the previous two decades, StarCraft I and II have been pioneering and enduring e-sports, 2 with millions of casual and highly competitive professional players. Defeating top human players therefore becomes a meaningful and measurable long-term objective.
</p>

<p style="text-align: justify">
From a reinforcement learning perspective, StarCraft II also offers an unparalleled opportunity to explore many challenging new frontiers:
</p>

<ol>
  <li>It is a multi-agent problem in which several players compete for influence and resources. It is also multi-agent at a lower-level: each player controls hundreds of units, which need to collaborate to achieve a common goal.</li>
  <li>It is an imperfect information game. The map is only partially observed via a local camera, which must be actively moved in order for the player to integrate.</li>
</ol>

<h3 id="pysc2-environment">PySC2 Environment</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="logo_sc2.png" width="200" alt="" /></th>
      <th style="text-align: center"><img src="logo_deepmind.png" width="200" alt="" /></th>
      <th style="text-align: center"><img src="logo_python.png" width="200" alt="" /></th>
      <th style="text-align: center"><img src="logo_tensorflow.png" width="200" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">StarCarft II</td>
      <td style="text-align: center">Google DeepMind</td>
      <td style="text-align: center">Python</td>
      <td style="text-align: center">TensorFlow</td>
    </tr>
  </tbody>
</table>

<p style="text-align: justify">
PySC2 is DeepMind's Python component of the StarCraft II Learning Environment (<strong>SC2LE</strong>). It exposes Blizzard Entertainment's StarCraft II Machine Learning API as a Python reinforcement learning (<strong>RL</strong>) Environment. 
This is a collaboration between DeepMind and Blizzard to develop StarCraft II into a rich environment for RL research. PySC2 provides an interface for RL agents to interact with StarCraft 2, getting observations and rewards and sending actions.
</p>

<p style="text-align: justify">
The image below explains how SC2LE works combining StarCarft II API with Google DeepMind Libraries:
</p>

<p align="center">
  <img src="sc2le.png" width="1000" alt="" />
  <br />
  <br />
  Fig. 1: SC2LE. Source: [1].
</p>

<p style="text-align: justify">
The two important components of the envrionment are: 
<ul>
  <li> Mini-game maps 
  <li> Replays 

</p>

<h3 id="objectives">Objectives</h3>

<p style="text-align: justify">
In Starcraft 2, no matter how good you are at combat or economy maximization, you cannot win a game without judicious selection of units, structures and buildings. We want to tackle the problem of build order optimization. Concretely, build order consists of finding concurrent action sequences constrained by unit dependencies and resource availability, such as creating a certain number of units and structures in the shortest amount of time possible.

</p>


<h2 id="techniques">Background Knowledge</h2>


<p style="text-align: justify">
Before starting to train a SC2 agent in the PySC2 environment, we went through a series of <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0">tutorials</a>, which implement reinforcement learning agents using <a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">Pytorch</a> and the <a href="https://gym.openai.com/envs/">OpenAI gym</a> environment.
</p>

<p style="text-align: justify">
Essential Algorithms: 
</p>

<ul>
  <li><a href="http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/">Demystifying deep reinforcement learning</a></li>
  <li><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0">Q-Learning with Tables and Neural Networks</a></li>
  <li><a href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149">Two-armed Bandit</a></li>
  <li><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c">Contextual Bandits</a></li>
  <li><a href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724">Policy-based Agents</a></li>
  <li><a href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99">Model-Based RL</a></li>
  <li><a href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df">Deep Q-Networks and Beyond</a></li>
  <li><a href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a">Visualizing an Agent’s Thoughts and Actions</a></li>
  <li><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc">Partial Observability and Deep Recurrent Q-Networks</a></li>
  <li><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf">Action-Selection Strategies for Exploration</a></li>
  <li><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2">Asynchronous Actor-Critic Agents (A3C)</a></li>
</ul>

<h3 id="a3c">Q-Learning</h3>

<p style="text-align: justify">
  Q(s,a) is equal to the summation of immediate reward after performing action a while in state s and the discounted expected future reward. It represents how good a certain action is in given state.


  Q-learning agent seeks to maximize its total (future) reward by adding the maximum expected reward attainable from the future state to the reward in its current state.


<p style="text-align: justify">Here are some implementations of Q-learning agents: <a href="https://chatbotslife.com/building-a-smart-pysc2-agent-cdc269cb095d"> Steven Brown's</a> and <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents-notebooks"> Morvan Zhou's </a>.</p></p>

<h3 id="a3c">Deep Q-Network</h3>

<p style="text-align: justify">


    DQN's main innovation over Q-learning is that it has two separate networks: an action network and a target network. The action network selects action, its parameters are trainable.The target network is a history version of the action network, its parameters are untrainable. It is updated infrequently– every C steps.The target network computes the loss for every action during training.
    Why not use just use one network for both estimations? The issue is that at every step of training, the Q-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control.  In order to make training more stable, the target network’s weights are fixed, and only periodically or slowly updated to the primary Q-networks values.



<p style="text-align: justify">Check out this tutorial on   <a href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"> how to use PyTorch to train a DQN agent </a>.  
</p></p>


<h4 id="theoretical-foundations">From Q-Learning to DQN</h4>



<p style="text-align: justify">
 Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly. our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state.

</p>

<p style="text-align: justify">
In order to go from Q-learning to DQN, these are the following things we implement:
<ul>
<li>Going from a single-layer network to a multi-layer convolutional network. </li>
<li> Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience.</li>
<li> Utilizing a second “target” network, which we will use to compute target Q-values during our updates.</li>
</ul>

</p>

<p style="text-align: justify">
The transition is depicted here:
</p>

<p align="center">
  <img src="table.png" alt="" />
  <br />
  <br />
  Fig. 2: Getting from Q-Network to Deep Q-Network. Source [4].
</p>


<h2 id="training-rl-agents">Training RL agents</h2>

<h3 id="objective">Objective</h3>

<p style="text-align: justify">
Inspired by Atari-Net, Our project addresses build order problem with a reinforcement learning approach. 
</p>

<p align="center">
  <img src="Atari-net.png" width="400" alt="" />
  <br />
  <br />
  Fig. 3: Atari-Net. Source [1].
</p>

<p style="text-align: justify"> Our initial project proposal focused on using reinforcement learning to optimize the problem of build-order optimization in the game of StarCraft II. Throughout the quarter, we experimented with different approaches to achieve some results that no other existing published bot has achieved. Given the time and resources we have available, our definition of success rested on the fact that our bot could beat the easy built-in StarCraft II AI on a minimap while running Terran against a fixed race. After a quarter of trial and error, we obtained substantial results by improving the <a href="https://chatbotslife.com/building-a-basic-pysc2-agent-b109cde1477c">refined agent </a> published by Steven Brown. 
</p>

<h3 id="initial-set-up-and-results">Our agents</h3>

<h2><a href="https://github.com/okdolly/SwaggerBot-StarCraft2/blob/master/comprehensiveAgent/SWARMBOT/supremebot.py">Supreme Bot</a></h2>


<p style="text-align: justify">
Refined agent uses the PySC2 AI environment for an RL agent using a Q-Table from having a win rate of less than 20% to, eventually, a win rate of over 70%. We delineated from the other choices of ML models and resorted to building a comprehensive agent that learns as it plays against the built-in AI in real time. Compared to the refined agent which Steven Brown uses to achieve the 70%+ win rate, our agent uses the Q-Table actions of building tech lab, which allows production units to build advanced army units, and reactors, which “ lengthens the build queue by three slots and allows two units to be built simultaneously” (starcraft wikia). 


</p>

<p align="center">
  <img src="images/dm_vs_hu180k.png" width="900" alt="" />
  <br />
  <br />
  Fig. 4: (Left) Results after 7 days of training vs (Right) DeepMind results. At the right hand side, the colors represent: (1) Light red, Atari-Net; (2) Light blue, Fully Conv.; (3) Dark blue, Fully Conv. with LSTM.
</p>



<h2><a href="https://github.com/okdolly/SwaggerBot-StarCraft2/blob/master/pytorch_dqn_agent/dqn-move-agent.py">DQN Bot</a></h2>


<p style="text-align: justify">
PySC2 api provides critical information, such as reward and episode,  an agent gets after performing an action, which makes DQN especially suitable for the task. A Starcraft RL agent observes the current state of the environment and chooses an action, the environment transitions to a new state, and also returns a reward that indicates the consequences of the action.  We encode six pieces of information as our state: whether the player can move, where the player is located, supply depot count,barracks count, supply limit and army supply.We passed the state into the neural network, which, after training, would return an action that yields the highest expected reward. 

</p>

<p style="text-align: justify">
A highlight of DQN is the usage of experience replay. We created a replay memory class that  stores sequences of state, action, next state, and rewards that the agent observes. By sampling from the replay memory randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure. The magic of DQN happens inside the ‘optimize’ method, which performs a single step of the optimization. It first samples a batch, concatenates all the tensors into a single one, computes Q(st,at) and V(st+1)=maxaQ(st+1,a), and combines them into our loss. Aside from the neural network, we hard-coded 9 actions and conditions to perform certain action into the DQN bot.
</p>

<p align="center">
  <img src="agen2M.png" width="900" alt="Agent2" />
</p>


<p align="center">
  <img src="agent2.png" width="900" alt="Agent2" />
</p>



<h2 id="next-step-hacking-the-reward">Evaluation</h2>




<h2 id="results-and-conclusions">Results and Conclusions</h2>

<p style="text-align: justify">
Due to constraint of time and resources, we were not achieved Deep Mind's performance. We focused more on reading research paper, learning about RL agorithms, doing tutorials rather than constructing new techniques from beginning.  
</p>

<h2> Journey </h2>
<p style="text-align: justify"> In the quater long project, we came across several challenges and new ideas. This is a brief of our journey: One of our original course of actions was to use parsed replay data to train the agent using NNs to learn strategies. It would intake information about the state and output abstracted actions that the agent would perform. However, that was not realistic because we faced troubles parsing the data in the right way. Primarily, the initial data set of raw replays we found, the SC2LE dataset, was not encoded correctly for the updated version of SC2. Using sc2reader, we were able to correctly parse replay data only for raw replays that were saved before early 2016. We were able to collect slightly over 1,000 parsed replays amongst all 3 races using the sc2reader library. This however, took a huge portion of our time, and once we started working on implementing the NN it was already week 6. Due to the time constraint and the fact that we lacked a working agent, we did not have a concrete plan of how to map the actions returned by the replay data and those of the agent. At this point we decided to split the work, where some of us continued to implement a NN, while others began implementing a DQN implementation. 
With the NN, we decided to make it Recurrent given the idea that an agent should learn what to do at specific states in the game based on previous states that happened. We tried to implement an LSTM RNN using Pytorch and also Tensorflow, but we ran into trouble when dealing with the dimensionality of the input since we had data that all had different number of states, ranging from 100 states in the game to 5000 states. We tried methods such as batching the data and padding the states with 0’s in order to input the same size of states per batch. We reached the point where the RNN was capable of producing a forward pass but unable to correctly perform  back propagation. Unfortunately, this was very late in the quarter (week 8) and our DQN implementation was working better, but still not fully. So in order to finish before the given deadline, we had to drop the RNN and focus all our efforts on finishing the agent using Q-Table, until we managed to get it to work. The time we had left to train it however, was not enough for it to learn fully. 

</p>


<p style="text-align: justify">
Overall, it was a fun and challenging project to learn about artificial intelligence. In the future, we plan to continue researching this field and learn more about it. 
</p>
<p>This project was developed during the <a href="https://github.com/dr-jam/ECS170">ECS 170:Artificial Intelligence</a> Course at UC Davis, Spring 2018.</p>
<pstyle="text-align: justify"> Thank you <a href="https://faculty.engineering.ucdavis.edu/mccoy/">Professor Josh McCoy</a> for giving us a great introduction to world of AI. 
 </p> 


<iframe width="560" height="315" src="https://www.youtube.com/embed/i_WQFI7Qol0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<h3 id="example-video-of-a-trained-agent">Example video of a trained agent</h3>
<p><a href="https://www.youtube.com/embed/i_WQFI7Qol0 " target="_blank"><img align="center" src="http://img.youtube.com/vi/uMiJt4UcZyo/0.jpg" alt="RL Agents" width="240" height="180" border="10" /></a></p>

<h2 id="references">References</h2>

<ol>
  <li>Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., … Tsing, R. (2017). StarCraft II: A New Challenge for Reinforcement Learning, <a href="https://doi.org/https://deepmind.com/documents/110/sc2le.pdf">link</a></li>
  <li>Simple Reinforcement Learning with Tensorflow series, Arthur Juliani, <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0">link</a> (Accessed: 2017/12/11)</li>
  <li>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. ICML, 2016, <a href="http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf">link</a></li>
  <li>Szepesvári, C. (2010). Algorithms for reinforcement learning. Synthesis lectures on artificial intelligence and machine learning, <a href="http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009">link</a></li>
  <li>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning, 1–9, <a href="https://doi.org/10.1038/nature14236">link</a></li>
</ol>


    </section>

    
  </body>
</html>